{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel 3 Data Streaming\n",
    "\n",
    "This service offers compressed Sentinel 3 NDVI data and makes it available via a high quality and memory efficient streaming solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "### DestinE Platform Credentials\n",
    "\n",
    "You need to have an account on the [Destination Earth Platform](https://auth.destine.eu/realms/desp/account).\n",
    "\n",
    "#### ⚠️ Warning: Authorized Access Only\n",
    "The usage of this notebook and data access is reserved only to authorized user groups.\n",
    "\n",
    "## Access the Data\n",
    "With a DESP account you can access the stream data proposed in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "%run ./auth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cap.stdout.split('\\n')\n",
    "refresh_token = output[1]\n",
    "token = output[2]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and general definitions\n",
    "We start by importing necessary packages and definitions regarding the resolution and the endpoint to the streaming api.\n",
    "\n",
    "Note: The API token must be set here including the user group. This happens in **Authentification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "jJMMfb6qeT"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import xarray as xa\n",
    "import rioxarray  # noqa\n",
    "from rasterio.transform import from_bounds\n",
    "from dtelib_s3 import DTEStreamer_S3\n",
    "import gc\n",
    "\n",
    "FORMAT = '%Y-%m-%dT%H:%M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for stream access\n",
    "\n",
    "Here the parameters are set to access the data from the service.\n",
    "\n",
    "*variable*: an abbreviated variable for the data</br>\n",
    "*program_subset*: a name for the program subest for the data</br>\n",
    "*start_date*: the time and date to start the stream</br>\n",
    "*end_date*: the time and date to end the stream</br>\n",
    "</br>\n",
    "\n",
    "To select a stream, chose parameter values from the table above, or if you have a *code snippet*, use it to replace the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continent = \"Europe\"\n",
    "\n",
    "start_date = datetime.strptime(\"2021-01-01T00:00\", FORMAT)\n",
    "end_date = datetime.strptime(\"2022-03-01T00:00\", FORMAT)\n",
    "\n",
    "save_path = \"./geotiffs\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the stream\n",
    "\n",
    "With the DTEStreamer class we can easily access the data stream through the api and access individual data frames. \n",
    "At first, we create a DTEStreamer object with the parameters defined in the step above. The object initializes by calling the api to get meta information about the stream and the location of the stream. (You can take a look at the api yourself in the swagger [here](https://streamer.destine.eu/api/v1/)). Also, ffmpeg is used to seek to the first image of the stream, according to *start_date*.\n",
    "\n",
    "The load_next_image() method then loads the next image into a numpy array, which is stored in the list *time_series* for further use.\n",
    "\n",
    "Note that in this example, the data and time stamps are loaded into a list. A print statements keep us on track with the progress.\n",
    "\n",
    "Note: This example should be modified to your purpose especially if you plan to do a long time series analysis, as it will load all the data of the loop into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = DTEStreamer_S3(continent=continent, start_date=start_date, end_date=end_date, token=token)\n",
    "\n",
    "for image, image_info in streamer.load_next_image():\n",
    "\n",
    "    da = xa.DataArray(image,\n",
    "                      dims=['latitude', 'longitude'],\n",
    "                      name=\"NDVI\"\n",
    "                     )\n",
    "\n",
    "    transform = from_bounds(*streamer.bbox(), streamer.nx(), streamer.ny())\n",
    "\n",
    "    da = da.rio.write_crs(\"EPSG:4326\")\n",
    "    da = da.rio.write_transform(transform)\n",
    "    \n",
    "    ft_stamp = image_info[\"img_date\"].strftime(\"%Y%m%d\")\n",
    "    satellite = image_info[\"satellite\"]\n",
    "    \n",
    "    tif_name = f'{save_path}/NDVI_{continent}_{satellite}_{ft_stamp}.tif'\n",
    "    da.rio.to_raster(tif_name, num_threads='all_cpus')\n",
    "    print(tif_name)\n",
    "    \n",
    "    del da\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
